#!/usr/bin/env python3
"""
Script t·∫°m ƒë·ªÉ scan c√°c kh√≥a h·ªçc, b√†i h·ªçc, subtitle ƒë√£ upload
- Scan Google Drive qua rclone
- Ki·ªÉm tra database (worker_rq.py ƒë√£ upload)
- T√¨m c√°c file JSON
"""

import subprocess
import json
import os
import sys
from datetime import datetime
from dotenv import load_dotenv
import mysql.connector
from collections import defaultdict

# Load environment variables
env_paths = [
    os.path.join(os.path.dirname(__file__), '../.env'),
    os.path.join(os.path.dirname(__file__), '../../.env'),
]
env_path = next((p for p in env_paths if os.path.exists(p)), None)
if env_path:
    load_dotenv(dotenv_path=env_path)

# Configuration
RCLONE_REMOTE = "gdrive"
RCLONE_DEST_PATH = "UdemyCourses/download_khoahoc"

# Database config
DB_CONFIG = {
    'user': os.getenv('DB_USER'),
    'password': os.getenv('DB_PASSWORD'),
    'host': os.getenv('DB_HOST'),
    'database': os.getenv('DB_NAME'),
}

def log(msg):
    """Log v·ªõi timestamp"""
    print(f"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] {msg}")

def check_rclone():
    """Ki·ªÉm tra rclone c√≥ s·∫µn kh√¥ng"""
    try:
        subprocess.run(['rclone', 'version'], check=True, capture_output=True)
        return True
    except (subprocess.CalledProcessError, FileNotFoundError):
        return False

def scan_rclone_uploads():
    """Scan c√°c kh√≥a h·ªçc ƒë√£ upload qua rclone"""
    log("=" * 80)
    log("SCANNING GOOGLE DRIVE VIA RCLONE")
    log("=" * 80)
    
    if not check_rclone():
        log("‚ùå rclone kh√¥ng ƒë∆∞·ª£c c√†i ƒë·∫∑t ho·∫∑c kh√¥ng c√≥ trong PATH")
        return {}
    
    remote_path = f"{RCLONE_REMOTE}:{RCLONE_DEST_PATH}"
    log(f"üìÇ Remote path: {remote_path}")
    
    courses_info = {}
    
    try:
        # Use rclone lsf to get folder names directly (more reliable)
        # lsf returns just the folder names with trailing slash
        cmd = ['rclone', 'lsf', '--dirs-only', remote_path]
        result = subprocess.run(cmd, capture_output=True, text=True, check=True)
        
        if not result.stdout.strip():
            log("‚ö†Ô∏è  Kh√¥ng t√¨m th·∫•y folder n√†o trong Google Drive")
            return {}
        
        folder_lines = [l.strip() for l in result.stdout.strip().split('\n') if l.strip() and l.strip().endswith('/')]
        total_folders = len(folder_lines)
        log(f"\nüìö T√¨m th·∫•y {total_folders} folder(s):")
        
        for idx, folder_line in enumerate(folder_lines, 1):
            # Remove trailing slash to get folder name
            folder_name = folder_line.rstrip('/')
            log(f"  üìÅ [{idx}/{total_folders}] {folder_name}")
            
            # Scan files in this course folder
            course_path = f"{remote_path}/{folder_name}"
            courses_info[folder_name] = {
                'path': course_path,
                'chapters': [],
                'lessons': [],
                'subtitles': [],
                'json_files': []
            }
            
            # List all files recursively - rclone ls automatically lists recursively
            try:
                # rclone ls automatically lists recursively, no need for --recursive flag
                # Use --fast-list for better performance with Google Drive
                # Reduced timeout to 30s per folder to speed up overall scan
                cmd = ['rclone', 'ls', '--fast-list', course_path]
                files_result = subprocess.run(cmd, capture_output=True, text=True, timeout=30)
                
                if files_result.returncode == 0:
                    if files_result.stdout.strip():
                        # Parse rclone ls output: "12345 file/path/name.ext"
                        file_count = 0
                        for line in files_result.stdout.strip().split('\n'):
                            if not line.strip():
                                continue
                            
                            # Extract file path (everything after the first space)
                            parts = line.strip().split(None, 1)
                            if len(parts) >= 2:
                                file_path = parts[1]
                                file_name = os.path.basename(file_path)
                                file_ext = os.path.splitext(file_name)[1].lower()
                                
                                file_count += 1
                                
                                # Check for subtitles
                                if file_ext in ['.srt', '.vtt']:
                                    courses_info[folder_name]['subtitles'].append(file_path)
                                
                                # Check for JSON files
                                elif file_ext == '.json':
                                    courses_info[folder_name]['json_files'].append(file_path)
                                
                                # Check for video files (lessons)
                                elif file_ext in ['.mp4', '.mkv', '.avi', '.mov', '.webm', '.m4v']:
                                    courses_info[folder_name]['lessons'].append(file_path)
                                
                                # Check for chapter folders (sections)
                                if '/' in file_path:
                                    chapter_name = file_path.split('/')[0]
                                    if chapter_name not in courses_info[folder_name]['chapters']:
                                        courses_info[folder_name]['chapters'].append(chapter_name)
                        
                        if file_count > 0:
                            log(f"     ‚úì T√¨m th·∫•y {file_count} file(s), {len(courses_info[folder_name]['chapters'])} chapter(s), {len(courses_info[folder_name]['lessons'])} lesson(s), {len(courses_info[folder_name]['subtitles'])} subtitle(s)")
                        else:
                            log(f"     ‚ö†Ô∏è  Folder tr·ªëng")
                    else:
                        log(f"     ‚ö†Ô∏è  Folder tr·ªëng ho·∫∑c kh√¥ng c√≥ file")
                else:
                    # Try without --fast-list as fallback (some rclone versions may not support it)
                    log(f"     ‚ö†Ô∏è  L·ªói v·ªõi --fast-list, th·ª≠ l·∫°i kh√¥ng d√πng flag n√†y...")
                    cmd = ['rclone', 'ls', course_path]
                    files_result = subprocess.run(cmd, capture_output=True, text=True, timeout=30)
                    
                    if files_result.returncode == 0 and files_result.stdout.strip():
                        file_count = 0
                        for line in files_result.stdout.strip().split('\n'):
                            if not line.strip():
                                continue
                            
                            parts = line.strip().split(None, 1)
                            if len(parts) >= 2:
                                file_path = parts[1]
                                file_name = os.path.basename(file_path)
                                file_ext = os.path.splitext(file_name)[1].lower()
                                
                                file_count += 1
                                
                                if file_ext in ['.srt', '.vtt']:
                                    courses_info[folder_name]['subtitles'].append(file_path)
                                elif file_ext == '.json':
                                    courses_info[folder_name]['json_files'].append(file_path)
                                elif file_ext in ['.mp4', '.mkv', '.avi', '.mov', '.webm', '.m4v']:
                                    courses_info[folder_name]['lessons'].append(file_path)
                                
                                if '/' in file_path:
                                    chapter_name = file_path.split('/')[0]
                                    if chapter_name not in courses_info[folder_name]['chapters']:
                                        courses_info[folder_name]['chapters'].append(chapter_name)
                        
                        if file_count > 0:
                            log(f"     ‚úì T√¨m th·∫•y {file_count} file(s), {len(courses_info[folder_name]['chapters'])} chapter(s), {len(courses_info[folder_name]['lessons'])} lesson(s), {len(courses_info[folder_name]['subtitles'])} subtitle(s)")
                    else:
                        error_msg = files_result.stderr.strip() if files_result.stderr else "Unknown error"
                        log(f"     ‚ùå L·ªói: {error_msg[:100]}")
            
            except subprocess.TimeoutExpired:
                log(f"     ‚ö†Ô∏è  Timeout khi scan folder (qu√° 30s) - b·ªè qua")
            except subprocess.CalledProcessError as e:
                error_detail = e.stderr.strip() if e.stderr else str(e)
                log(f"     ‚ùå L·ªói rclone: {error_detail[:150]}")
            except Exception as e:
                log(f"     ‚ùå L·ªói kh√¥ng mong ƒë·ª£i: {str(e)[:150]}")
    
    except subprocess.CalledProcessError as e:
        log(f"‚ùå L·ªói khi list folders t·ª´ rclone: {e}")
        log(f"   Output: {e.stderr}")
        return {}
    except Exception as e:
        log(f"‚ùå L·ªói kh√¥ng mong ƒë·ª£i: {e}")
        return {}
    
    return courses_info

def scan_database_uploads():
    """Scan c√°c task ƒë√£ upload qua worker_rq.py t·ª´ database"""
    log("\n" + "=" * 80)
    log("SCANNING DATABASE (worker_rq.py uploads)")
    log("=" * 80)
    
    tasks_info = []
    
    try:
        conn = mysql.connector.connect(**DB_CONFIG)
        cursor = conn.cursor(dictionary=True)
        
        # Get all completed tasks with drive_link
        query = """
            SELECT 
                id, 
                order_id, 
                email, 
                course_url, 
                title, 
                status, 
                drive_link, 
                created_at, 
                updated_at
            FROM download_tasks
            WHERE status IN ('completed', 'enrolled')
            AND drive_link IS NOT NULL
            AND drive_link != ''
            ORDER BY updated_at DESC
        """
        
        cursor.execute(query)
        tasks = cursor.fetchall()
        
        log(f"\nüìä T√¨m th·∫•y {len(tasks)} task(s) ƒë√£ ho√†n th√†nh v·ªõi drive_link:")
        
        for task in tasks:
            log(f"  ‚úÖ Task #{task['id']}: {task['title'] or 'N/A'}")
            log(f"     Email: {task['email']}")
            log(f"     Drive Link: {task['drive_link']}")
            log(f"     Updated: {task['updated_at']}")
            
            tasks_info.append({
                'task_id': task['id'],
                'order_id': task['order_id'],
                'email': task['email'],
                'title': task['title'],
                'course_url': task['course_url'],
                'drive_link': task['drive_link'],
                'status': task['status'],
                'created_at': str(task['created_at']),
                'updated_at': str(task['updated_at'])
            })
        
        cursor.close()
        conn.close()
        
    except mysql.connector.Error as e:
        log(f"‚ùå L·ªói database: {e}")
        return []
    except Exception as e:
        log(f"‚ùå L·ªói kh√¥ng mong ƒë·ª£i: {e}")
        return []
    
    return tasks_info

def scan_json_files():
    """Scan c√°c file JSON trong th∆∞ m·ª•c saved"""
    log("\n" + "=" * 80)
    log("SCANNING JSON FILES")
    log("=" * 80)
    
    json_files = []
    
    # Check saved directory in udemy_dl
    saved_dir = os.path.join(os.path.dirname(__file__), '../udemy_dl/saved')
    
    if os.path.exists(saved_dir):
        log(f"üìÇ Scanning: {saved_dir}")
        
        for root, dirs, files in os.walk(saved_dir):
            for file in files:
                if file.endswith('.json'):
                    file_path = os.path.join(root, file)
                    json_files.append(file_path)
                    log(f"  üìÑ {file_path}")
    
    # Check for course_content.json and _udemy.json in udemy_dl root
    udemy_dl_dir = os.path.join(os.path.dirname(__file__), '../udemy_dl')
    for json_file in ['course_content.json', '_udemy.json']:
        json_path = os.path.join(udemy_dl_dir, json_file)
        if os.path.exists(json_path):
            json_files.append(json_path)
            log(f"  üìÑ {json_path}")
    
    if not json_files:
        log("‚ö†Ô∏è  Kh√¥ng t√¨m th·∫•y file JSON n√†o")
    
    return json_files

def generate_summary(rclone_data, db_tasks, json_files):
    """T·∫°o b√°o c√°o t·ªïng h·ª£p"""
    log("\n" + "=" * 80)
    log("üìä T·ªîNG H·ª¢P B√ÅO C√ÅO")
    log("=" * 80)
    
    # Rclone summary
    total_courses_rclone = len(rclone_data)
    total_lessons_rclone = sum(len(course['lessons']) for course in rclone_data.values())
    total_subtitles_rclone = sum(len(course['subtitles']) for course in rclone_data.values())
    total_chapters_rclone = sum(len(course['chapters']) for course in rclone_data.values())
    total_json_rclone = sum(len(course['json_files']) for course in rclone_data.values())
    
    log(f"\nüìö GOOGLE DRIVE (RCLONE):")
    log(f"   ‚Ä¢ T·ªïng s·ªë kh√≥a h·ªçc: {total_courses_rclone}")
    log(f"   ‚Ä¢ T·ªïng s·ªë chapter: {total_chapters_rclone}")
    log(f"   ‚Ä¢ T·ªïng s·ªë b√†i h·ªçc (video): {total_lessons_rclone}")
    log(f"   ‚Ä¢ T·ªïng s·ªë subtitle: {total_subtitles_rclone}")
    log(f"   ‚Ä¢ T·ªïng s·ªë file JSON: {total_json_rclone}")
    
    # Database summary
    log(f"\nüíæ DATABASE (worker_rq.py):")
    log(f"   ‚Ä¢ T·ªïng s·ªë task ƒë√£ ho√†n th√†nh: {len(db_tasks)}")
    
    # Group by status
    status_count = defaultdict(int)
    for task in db_tasks:
        status_count[task['status']] += 1
    
    for status, count in status_count.items():
        log(f"   ‚Ä¢ Status '{status}': {count} task(s)")
    
    # JSON files summary
    log(f"\nüìÑ JSON FILES:")
    log(f"   ‚Ä¢ T·ªïng s·ªë file JSON: {len(json_files)}")
    
    # Detailed course breakdown
    if rclone_data:
        log(f"\nüìã CHI TI·∫æT T·ª™NG KH√ìA H·ªåC:")
        for course_name, course_info in rclone_data.items():
            log(f"\n   üìö {course_name}:")
            log(f"      ‚Ä¢ Chapters: {len(course_info['chapters'])}")
            log(f"      ‚Ä¢ Lessons: {len(course_info['lessons'])}")
            log(f"      ‚Ä¢ Subtitles: {len(course_info['subtitles'])}")
            log(f"      ‚Ä¢ JSON files: {len(course_info['json_files'])}")
            
            # Show subtitle languages if available
            if course_info['subtitles']:
                subtitle_langs = set()
                for sub_path in course_info['subtitles']:
                    # Try to extract language from filename (format: LectureName_lang.srt)
                    parts = os.path.basename(sub_path).split('_')
                    if len(parts) >= 2:
                        lang = parts[-1].replace('.srt', '').replace('.vtt', '')
                        subtitle_langs.add(lang)
                
                if subtitle_langs:
                    log(f"      ‚Ä¢ Ng√¥n ng·ªØ subtitle: {', '.join(sorted(subtitle_langs))}")

def main():
    """H√†m ch√≠nh"""
    log("üöÄ B·∫Øt ƒë·∫ßu scan c√°c kh√≥a h·ªçc, b√†i h·ªçc, subtitle ƒë√£ upload...")
    log("")
    
    # 1. Scan rclone uploads
    rclone_data = scan_rclone_uploads()
    
    # 2. Scan database uploads
    db_tasks = scan_database_uploads()
    
    # 3. Scan JSON files
    json_files = scan_json_files()
    
    # 4. Generate summary
    generate_summary(rclone_data, db_tasks, json_files)
    
    log("\n" + "=" * 80)
    log("‚úÖ Ho√†n th√†nh scan!")
    log("=" * 80)

if __name__ == "__main__":
    main()

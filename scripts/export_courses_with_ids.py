#!/usr/bin/env python3
"""
Script xu·∫•t t·∫•t c·∫£ kh√≥a h·ªçc v·ªõi c·∫•u tr√∫c ph√¢n c·∫•p v√† Google Drive ID
- M·ªói kh√≥a h·ªçc ch·ª©a c√°c ch∆∞∆°ng v√† b√†i h·ªçc/subtitle/t√†i li·ªáu
- C√≥ ph√¢n c·∫•p r√µ r√†ng
- C√≥ ID Google Drive cho t·ª´ng file v√† kh√≥a h·ªçc
"""

import subprocess
import json
import os
import sys
from datetime import datetime
from dotenv import load_dotenv
from collections import defaultdict

# Load environment variables
env_paths = [
    os.path.join(os.path.dirname(__file__), '../.env'),
    os.path.join(os.path.dirname(__file__), '../../.env'),
]
env_path = next((p for p in env_paths if os.path.exists(p)), None)
if env_path:
    load_dotenv(dotenv_path=env_path)

# Configuration
RCLONE_REMOTE = "gdrive"
RCLONE_DEST_PATH = "UdemyCourses/download_khoahoc"
OUTPUT_FILE = "courses_export.json"

def log(msg):
    """Log v·ªõi timestamp"""
    print(f"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] {msg}")

def check_rclone():
    """Ki·ªÉm tra rclone c√≥ s·∫µn kh√¥ng"""
    try:
        subprocess.run(['rclone', 'version'], check=True, capture_output=True)
        return True
    except (subprocess.CalledProcessError, FileNotFoundError):
        return False

def get_file_type(file_name, mime_type):
    """X√°c ƒë·ªãnh lo·∫°i file"""
    file_ext = os.path.splitext(file_name)[1].lower()
    
    # Video files
    if file_ext in ['.mp4', '.mkv', '.avi', '.mov', '.webm', '.m4v']:
        return 'video'
    # Subtitle files
    elif file_ext in ['.srt', '.vtt']:
        return 'subtitle'
    # Document files
    elif file_ext in ['.pdf', '.doc', '.docx', '.txt', '.html']:
        return 'document'
    # JSON files
    elif file_ext == '.json':
        return 'json'
    # Other files
    else:
        return 'other'

def scan_course_with_structure(course_path, course_name):
    """Scan m·ªôt kh√≥a h·ªçc v·ªõi c·∫•u tr√∫c ph√¢n c·∫•p ƒë·∫ßy ƒë·ªß"""
    log(f"  üìÅ ƒêang scan: {course_name}")
    
    course_data = {
        'course_name': course_name,
        'course_path': course_path,
        'course_id': None,
        'chapters': []
    }
    
    try:
        # L·∫•y th√¥ng tin folder kh√≥a h·ªçc (ƒë·ªÉ l·∫•y ID)
        cmd = ['rclone', 'lsjson', '--dirs-only', course_path]
        result = subprocess.run(cmd, capture_output=True, text=True, timeout=10)
        
        if result.returncode == 0:
            try:
                folders = json.loads(result.stdout)
                # T√¨m folder ch√≠nh (c√≥ th·ªÉ l√† ch√≠nh n√≥ ho·∫∑c folder con)
                for folder in folders:
                    if folder.get('IsDir') and folder.get('Path') == '.' or not folder.get('Path'):
                        course_data['course_id'] = folder.get('ID')
                        break
            except json.JSONDecodeError:
                pass
        
        # L·∫•y t·∫•t c·∫£ files v√† folders v·ªõi metadata
        cmd = ['rclone', 'lsjson', '--recursive', course_path]
        result = subprocess.run(cmd, capture_output=True, text=True, timeout=60)
        
        if result.returncode != 0:
            log(f"     ‚ö†Ô∏è  Kh√¥ng th·ªÉ l·∫•y metadata t·ª´ rclone")
            return course_data
        
        try:
            items = json.loads(result.stdout)
        except json.JSONDecodeError as e:
            log(f"     ‚ùå L·ªói parse JSON: {e}")
            return course_data
        
        # T·ªï ch·ª©c d·ªØ li·ªáu theo c·∫•u tr√∫c ph√¢n c·∫•p
        chapters_dict = defaultdict(lambda: {
            'chapter_name': '',
            'chapter_id': None,
            'chapter_path': '',
            'lessons': [],
            'subtitles': [],
            'documents': [],
            'other_files': []
        })
        
        for item in items:
            path = item.get('Path', '')
            name = item.get('Name', '')
            item_id = item.get('ID', '')
            is_dir = item.get('IsDir', False)
            size = item.get('Size', 0)
            mime_type = item.get('MimeType', '')
            
            # B·ªè qua folder g·ªëc
            if path == '.' or not path:
                if is_dir and not course_data['course_id']:
                    course_data['course_id'] = item_id
                continue
            
            # X√°c ƒë·ªãnh chapter (folder ƒë·∫ßu ti√™n trong path)
            if '/' in path:
                chapter_name = path.split('/')[0]
                file_path = '/'.join(path.split('/')[1:]) if len(path.split('/')) > 1 else ''
            else:
                # File ·ªü root level
                chapter_name = '_root'
                file_path = path
            
            # Kh·ªüi t·∫°o chapter n·∫øu ch∆∞a c√≥
            if chapter_name not in chapters_dict:
                # T√¨m chapter ID t·ª´ danh s√°ch folders
                chapter_id = None
                for folder_item in items:
                    if folder_item.get('IsDir') and folder_item.get('Path') == chapter_name:
                        chapter_id = folder_item.get('ID')
                        break
                
                chapters_dict[chapter_name] = {
                    'chapter_name': chapter_name,
                    'chapter_id': chapter_id,
                    'chapter_path': chapter_name,
                    'lessons': [],
                    'subtitles': [],
                    'documents': [],
                    'other_files': []
                }
            
            # N·∫øu l√† file (kh√¥ng ph·∫£i folder)
            if not is_dir:
                file_type = get_file_type(name, mime_type)
                file_info = {
                    'file_name': name,
                    'file_id': item_id,
                    'file_type': file_type,
                    'file_size': size,
                    'file_path': path,
                    'mime_type': mime_type
                }
                
                if file_type == 'video':
                    chapters_dict[chapter_name]['lessons'].append(file_info)
                elif file_type == 'subtitle':
                    chapters_dict[chapter_name]['subtitles'].append(file_info)
                elif file_type == 'document':
                    chapters_dict[chapter_name]['documents'].append(file_info)
                else:
                    chapters_dict[chapter_name]['other_files'].append(file_info)
        
        # Chuy·ªÉn t·ª´ dict sang list v√† s·∫Øp x·∫øp
        chapters_list = []
        for chapter_name in sorted(chapters_dict.keys()):
            chapter = chapters_dict[chapter_name]
            # S·∫Øp x·∫øp c√°c files trong chapter
            chapter['lessons'].sort(key=lambda x: x['file_name'])
            chapter['subtitles'].sort(key=lambda x: x['file_name'])
            chapter['documents'].sort(key=lambda x: x['file_name'])
            chapter['other_files'].sort(key=lambda x: x['file_name'])
            chapters_list.append(chapter)
        
        course_data['chapters'] = chapters_list
        
        # Th·ªëng k√™
        total_lessons = sum(len(ch['lessons']) for ch in chapters_list)
        total_subtitles = sum(len(ch['subtitles']) for ch in chapters_list)
        total_documents = sum(len(ch['documents']) for ch in chapters_list)
        
        log(f"     ‚úì {len(chapters_list)} chapter(s), {total_lessons} lesson(s), {total_subtitles} subtitle(s), {total_documents} document(s)")
        
    except subprocess.TimeoutExpired:
        log(f"     ‚ö†Ô∏è  Timeout khi scan")
    except Exception as e:
        log(f"     ‚ùå L·ªói: {str(e)[:150]}")
    
    return course_data

def scan_all_courses():
    """Scan t·∫•t c·∫£ kh√≥a h·ªçc"""
    log("=" * 80)
    log("SCANNING T·∫§T C·∫¢ KH√ìA H·ªåC V·ªöI C·∫§U TR√öC PH√ÇN C·∫§P")
    log("=" * 80)
    
    if not check_rclone():
        log("‚ùå rclone kh√¥ng ƒë∆∞·ª£c c√†i ƒë·∫∑t ho·∫∑c kh√¥ng c√≥ trong PATH")
        return []
    
    remote_path = f"{RCLONE_REMOTE}:{RCLONE_DEST_PATH}"
    log(f"üìÇ Remote path: {remote_path}")
    
    courses = []
    
    try:
        # L·∫•y danh s√°ch folders (kh√≥a h·ªçc)
        cmd = ['rclone', 'lsf', '--dirs-only', remote_path]
        result = subprocess.run(cmd, capture_output=True, text=True, check=True)
        
        if not result.stdout.strip():
            log("‚ö†Ô∏è  Kh√¥ng t√¨m th·∫•y folder n√†o trong Google Drive")
            return []
        
        folder_lines = [l.strip() for l in result.stdout.strip().split('\n') if l.strip() and l.strip().endswith('/')]
        total_folders = len(folder_lines)
        log(f"\nüìö T√¨m th·∫•y {total_folders} kh√≥a h·ªçc:")
        
        for idx, folder_line in enumerate(folder_lines, 1):
            folder_name = folder_line.rstrip('/')
            course_path = f"{remote_path}/{folder_name}"
            
            log(f"\n[{idx}/{total_folders}] {folder_name}")
            
            course_data = scan_course_with_structure(course_path, folder_name)
            courses.append(course_data)
        
    except subprocess.CalledProcessError as e:
        log(f"‚ùå L·ªói khi list folders t·ª´ rclone: {e}")
        log(f"   Output: {e.stderr}")
        return []
    except Exception as e:
        log(f"‚ùå L·ªói kh√¥ng mong ƒë·ª£i: {e}")
        return []
    
    return courses

def export_to_json(courses, output_file):
    """Xu·∫•t d·ªØ li·ªáu ra file JSON"""
    log("\n" + "=" * 80)
    log("XU·∫§T D·ªÆ LI·ªÜU RA FILE JSON")
    log("=" * 80)
    
    export_data = {
        'export_date': datetime.now().isoformat(),
        'total_courses': len(courses),
        'courses': courses
    }
    
    # T√≠nh t·ªïng s·ªë
    total_chapters = sum(len(course['chapters']) for course in courses)
    total_lessons = sum(sum(len(ch['lessons']) for ch in course['chapters']) for course in courses)
    total_subtitles = sum(sum(len(ch['subtitles']) for ch in course['chapters']) for course in courses)
    total_documents = sum(sum(len(ch['documents']) for ch in course['chapters']) for course in courses)
    
    export_data['statistics'] = {
        'total_courses': len(courses),
        'total_chapters': total_chapters,
        'total_lessons': total_lessons,
        'total_subtitles': total_subtitles,
        'total_documents': total_documents
    }
    
    try:
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(export_data, f, ensure_ascii=False, indent=2)
        
        file_size = os.path.getsize(output_file)
        log(f"\n‚úÖ ƒê√£ xu·∫•t th√†nh c√¥ng!")
        log(f"   üìÑ File: {output_file}")
        log(f"   üìä K√≠ch th∆∞·ªõc: {file_size:,} bytes ({file_size/1024/1024:.2f} MB)")
        log(f"   üìö T·ªïng s·ªë kh√≥a h·ªçc: {len(courses)}")
        log(f"   üìñ T·ªïng s·ªë ch∆∞∆°ng: {total_chapters}")
        log(f"   üé• T·ªïng s·ªë b√†i h·ªçc: {total_lessons}")
        log(f"   üìù T·ªïng s·ªë subtitle: {total_subtitles}")
        log(f"   üìÑ T·ªïng s·ªë t√†i li·ªáu: {total_documents}")
        
        return True
    except Exception as e:
        log(f"‚ùå L·ªói khi xu·∫•t file: {e}")
        return False

def main():
    """H√†m ch√≠nh"""
    log("üöÄ B·∫Øt ƒë·∫ßu scan v√† xu·∫•t t·∫•t c·∫£ kh√≥a h·ªçc v·ªõi Google Drive ID...")
    log("")
    
    # Scan t·∫•t c·∫£ kh√≥a h·ªçc
    courses = scan_all_courses()
    
    if not courses:
        log("\n‚ö†Ô∏è  Kh√¥ng t√¨m th·∫•y kh√≥a h·ªçc n√†o")
        return
    
    # Xu·∫•t ra file JSON
    output_path = os.path.join(os.path.dirname(__file__), OUTPUT_FILE)
    success = export_to_json(courses, output_path)
    
    if success:
        log("\n" + "=" * 80)
        log("‚úÖ Ho√†n th√†nh!")
        log("=" * 80)
        log(f"\nüìÑ File ƒë√£ ƒë∆∞·ª£c l∆∞u t·∫°i: {output_path}")
        log(f"   B·∫°n c√≥ th·ªÉ m·ªü file n√†y ƒë·ªÉ xem chi ti·∫øt t·∫•t c·∫£ kh√≥a h·ªçc v·ªõi c·∫•u tr√∫c ph√¢n c·∫•p v√† Google Drive ID")
    else:
        log("\n‚ùå C√≥ l·ªói x·∫£y ra khi xu·∫•t file")

if __name__ == "__main__":
    main()
